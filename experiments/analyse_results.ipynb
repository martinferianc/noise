{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from average_results import main\n",
    "from src.data import get_dataset_options\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tabulate\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "my_gradient = LinearSegmentedColormap.from_list('my_gradient', (\n",
    "    # Edit this gradient at https://eltos.github.io/gradient/#00B20F-82FB39-EAF0E0-FF3540-AC1E25\n",
    "    (0.000, (0.000, 0.698, 0.059)),\n",
    "    (0.250, (0.510, 0.984, 0.224)),\n",
    "    (0.500, (0.918, 0.941, 0.878)),\n",
    "    (0.750, (1.000, 0.208, 0.251)),\n",
    "    (1.000, (0.675, 0.118, 0.145))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('analysis_images'):\n",
    "    os.makedirs('analysis_images')\n",
    "if not os.path.exists('selected_performances'):\n",
    "    os.makedirs('selected_performances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define several helper functions to simplify the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(experiment_paths, as_file=False, root_dir='.'):\n",
    "    experiment_paths_list = experiment_paths.split()\n",
    "    # remove the version number from experiment_paths\n",
    "    # find location of last v[0-9] in each path\n",
    "    all_paths = os.listdir(root_dir)\n",
    "    experiment_paths_dict = {}\n",
    "    save_path_details = {}\n",
    "    for experiment_path in experiment_paths_list:\n",
    "        v_start = re.search(r'v[0-9]+', experiment_path).start() - 1\n",
    "        # find the full path of experiment_path and take the latest one if multiple exist\n",
    "        full_path = sorted([path for path in all_paths if experiment_path == path])[-1]\n",
    "        full_path = os.path.join(root_dir, full_path)\n",
    "\n",
    "        if experiment_path[:v_start] not in experiment_paths_dict:\n",
    "            experiment_paths_dict[experiment_path[:v_start]] = [full_path]\n",
    "            save_path_details[experiment_path[:v_start]] = {'dataset': full_path.split('-')[0],\n",
    "                                                            'architecture': full_path.split('-')[1]}\n",
    "        else:\n",
    "            experiment_paths_dict[experiment_path[:v_start]].append(full_path)\n",
    "\n",
    "    if as_file:\n",
    "        for label in experiment_paths_dict:\n",
    "            save_path = './runs/' + save_path_details[label]['dataset'] + '/' + save_path_details[label]['architecture'] + '/' + 'averages/' \n",
    "            main(experiment_paths_dict[label], label, save_path)\n",
    "    else:\n",
    "        results_dict = {}\n",
    "        for label in experiment_paths_dict:\n",
    "            results = main(experiment_paths_dict[label], label)\n",
    "            results_dict[label] = results\n",
    "\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_multiple_datasets(experiment_paths_groups, as_file=False, root_dir='.'):\n",
    "    group_results_dict = {}\n",
    "    for experiment_paths in experiment_paths_groups:\n",
    "        dataset_name = experiment_paths.split()[0].split('-')[0]\n",
    "        group_results_dict[dataset_name] = load_results(experiment_paths, as_file=as_file, root_dir=root_dir)\n",
    "\n",
    "    group_results_dict_formatted = {}\n",
    "    for dataset_name in group_results_dict:\n",
    "        dataset_dict = group_results_dict[dataset_name]\n",
    "        dataset_dict_formatted = {}\n",
    "        for label in dataset_dict:\n",
    "            if 'top' in label:\n",
    "                if 'top2-direct' in label:\n",
    "                    dataset_dict_formatted[label.replace('top2-direct', 'top2direct')] = dataset_dict[label]\n",
    "                elif 'top3-direct' in label:\n",
    "                    dataset_dict_formatted[label.replace('top3-direct', 'top3direct')] = dataset_dict[label]\n",
    "                elif 'top2' in label:\n",
    "                    dataset_dict_formatted[label.replace('top2', 'top2opt')] = dataset_dict[label]\n",
    "                elif 'top3' in label:\n",
    "                    dataset_dict_formatted[label.replace('top3', 'top3opt')] = dataset_dict[label]\n",
    "            else:\n",
    "                dataset_dict_formatted[label] = dataset_dict[label]\n",
    "        group_results_dict_formatted[dataset_name] = dataset_dict_formatted\n",
    "\n",
    "    return group_results_dict_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table_multiple_datasets(group_results_dict, metric=\"error\", show_std=True, num_dp=2, perc=True):\n",
    "    ordered_datasets = [\"svhn\", \"cifar10\", \"cifar100\", \"tinyimagenet\", \"newsgroup\", \"sst\", \"rotated_cifar100\", \"wiki_face\", \"regression_energy\", \"regression_boston\", \"regression_wine\", \"regression_yacht\", \"regression_concrete\", \"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]\n",
    "    present_datasets = []\n",
    "    id_tables = {}\n",
    "    ood_tables = {}\n",
    "    first_dataset = True\n",
    "\n",
    "    if metric == \"error\" or metric == \"ece\":\n",
    "        perc = True\n",
    "    else:\n",
    "        perc = False\n",
    "    \n",
    "    for dataset in ordered_datasets:\n",
    "        if dataset not in group_results_dict:\n",
    "            continue\n",
    "        present_datasets.append(dataset)\n",
    "        results_dict = group_results_dict[dataset]\n",
    "        assert dataset == list(results_dict.keys())[0].split('-')[0]\n",
    "\n",
    "        print()\n",
    "        label = '-'.join(list(results_dict.keys())[0].split('-')[:2])\n",
    "    \n",
    "        label_mapping = {\n",
    "        'vanilla': 'No Noise',\n",
    "        'target_smoothing': 'Label Smoothing',\n",
    "        'input_ods': 'Input ODS',\n",
    "        'input_augmix': 'Input AugMix',\n",
    "        'input_target_mixup': 'Input-Target MixUp',\n",
    "        'input_target_cmixup': 'Input-Target CMixUp',\n",
    "        'activation_dropout': 'Activation Dropout',\n",
    "        'gradient_gaussian': 'Gradient Gaussian',\n",
    "        'model_sp': 'Model',\n",
    "        'top2direct': 'Top-2 Direct',\n",
    "        'top3direct': 'Top-3 Direct', \n",
    "        'top2opt': 'Top-2 Optimised',\n",
    "        'top3opt': 'Top-3 Optimised',\n",
    "        'input_additive_gaussian': 'Input Gaussian',\n",
    "        'activation_additive_gaussian': 'Activation Gaussian',\n",
    "        'input_random_crop_horizontal_flip': 'Input Weak Aug.',\n",
    "        'weight_additive_gaussian': 'Weight Gaussian',\n",
    "        'weight_dropconnect': 'Weight DropConnect',\n",
    "        }\n",
    "\n",
    "        ordered_labels = ['No Noise', 'Input Weak Aug.', 'Input Gaussian', 'Input ODS', 'Input AugMix', 'Input-Target MixUp', 'Input-Target CMixUp', 'Label Smoothing', 'Activation Gaussian', 'Activation Dropout', 'Gradient Gaussian', 'Model', 'Weight Gaussian', 'Weight DropConnect', 'Top-2 Direct', 'Top-3 Direct', 'Top-2 Optimised', 'Top-3 Optimised']\n",
    "        \n",
    "        relabeled_results = {}\n",
    "        for key in results_dict:\n",
    "            for label_key in label_mapping:\n",
    "                if label_key in key:\n",
    "                    new_key = label_key\n",
    "            relabeled_results[label_mapping[new_key]] = results_dict[key]\n",
    "\n",
    "        # maintain the preferred order of labels\n",
    "        labels = []\n",
    "        for key in ordered_labels:\n",
    "            if key in relabeled_results:\n",
    "                labels.append(key)\n",
    "\n",
    "        results = relabeled_results\n",
    "\n",
    "        splits = [\"test\"]\n",
    "        _, _, _, levels, augmentation_types = get_dataset_options(dataset)\n",
    "        for i, augmentation in enumerate(augmentation_types):\n",
    "            for level in levels[i]:\n",
    "                splits += [\"{}_{}\".format(augmentation, level)]\n",
    "\n",
    "        # Cache the first table because it will be for the test split\n",
    "        first_table = None\n",
    "        for split in splits:\n",
    "            # Create a table which will compare all the methods on a split with the metric\n",
    "            # The first column will be the method names\n",
    "            table = [[label] for label in labels]\n",
    "\n",
    "            for label, result in results.items():\n",
    "                datapoint = result[metric][split]\n",
    "                if isinstance(datapoint, float):\n",
    "                    # This stands for no standard deviation, since the result was not averaged\n",
    "                    datapoint = (datapoint, 0.0)\n",
    "                if perc:\n",
    "                    datapoint = (datapoint[0]*100, datapoint[1]*100)\n",
    "                if show_std:\n",
    "                    if num_dp == 2:\n",
    "                        if len(\"{:.2f}\".format(datapoint[0])) < 5:\n",
    "                            table[labels.index(label)].append(\n",
    "                                \"\\phantom{0}\" + \"{:.2f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.2f}\".format(datapoint[1]) + \"}\")\n",
    "                        else:\n",
    "                            table[labels.index(label)].append(\n",
    "                                \"{:.2f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.2f}\".format(datapoint[1]) + \"}\")\n",
    "                    else:\n",
    "                        if len(\"{:.1f}\".format(datapoint[0])) < 4:\n",
    "                            table[labels.index(label)].append(\n",
    "                                \"\\phantom{0}\" + \"{:.1f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.1f}\".format(datapoint[1]) + \"}\")\n",
    "                        else:\n",
    "                            table[labels.index(label)].append(\n",
    "                                \"{:.1f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.1f}\".format(datapoint[1]) + \"}\")\n",
    "                else:\n",
    "                    if len(\"{:.2f}\".format(datapoint[0])) < 5:\n",
    "                        table[labels.index(label)].append(\n",
    "                            \"\\phantom{0}\"+\"{:.2f}\".format(datapoint[0]))\n",
    "                    else:\n",
    "                        table[labels.index(label)].append(\n",
    "                            \"{:.2f}\".format(datapoint[0]))\n",
    "                    \n",
    "            if first_table is None:\n",
    "                first_table = table\n",
    "        \n",
    "        if first_dataset:\n",
    "            id_tables[dataset] = first_table\n",
    "        else:\n",
    "            # remove the first column from the table\n",
    "            first_table = [row[1:] for row in first_table]\n",
    "            id_tables[dataset] = first_table\n",
    "        first_dataset = False\n",
    "\n",
    "        # This is going to aggregate all the augmentations together\n",
    "        # Again create a table which will compare all the methods on all the splits together with all the metrics\n",
    "        table = [[label] for label in labels]\n",
    "\n",
    "        splits = [\"ood_test\"]\n",
    "        for split in splits:\n",
    "            # Create a table which will compare all the methods on a split with the metric\n",
    "            # The first column will be the method names\n",
    "            table = [[label] for label in labels]\n",
    "\n",
    "            for label, result in results.items():\n",
    "                datapoint = result[metric][split]\n",
    "\n",
    "                if isinstance(datapoint, float):\n",
    "                    # This stands for no standard deviation, since the result was not averaged\n",
    "                    datapoint = (datapoint, 0.0)\n",
    "                if perc:\n",
    "                    datapoint = (datapoint[0]*100, datapoint[1]*100)\n",
    "                # check if nan\n",
    "                if datapoint[0] != datapoint[0]:\n",
    "                    table[labels.index(label)].append(\"N/A\")    \n",
    "                else:\n",
    "                    if show_std:\n",
    "                        if num_dp == 2:\n",
    "                            if len(\"{:.2f}\".format(datapoint[0])) < 5:\n",
    "                                table[labels.index(label)].append(\n",
    "                                    \"\\phantom{0}\" + \"{:.2f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.2f}\".format(datapoint[1]) + \"}\")\n",
    "                            else:\n",
    "                                table[labels.index(label)].append(\n",
    "                                    \"{:.2f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.2f}\".format(datapoint[1]) + \"}\")\n",
    "                        else:\n",
    "                            if len(\"{:.1f}\".format(datapoint[0])) < 4:\n",
    "                                table[labels.index(label)].append(\n",
    "                                    \"\\phantom{0}\" + \"{:.1f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.1f}\".format(datapoint[1]) + \"}\")\n",
    "                            else:\n",
    "                                table[labels.index(label)].append(\n",
    "                                    \"{:.1f}\".format(datapoint[0]) + \"\\\\tiny{$\\pm$\" + \"{:.1f}\".format(datapoint[1]) + \"}\")\n",
    "                    else:\n",
    "                        if len(\"{:.2f}\".format(datapoint[0])) < 5:\n",
    "                            table[labels.index(label)].append(\n",
    "                                \"\\phantom{0}\"+\"{:.2f}\".format(datapoint[0]))\n",
    "                        else:\n",
    "                            table[labels.index(label)].append(\n",
    "                                \"{:.2f}\".format(datapoint[0]))    \n",
    "\n",
    "        # Merge the last table with the first one\n",
    "        augmentation_table = table\n",
    "        # Remove the first column from the augmentation table\n",
    "        augmentation_table = [row[1:] for row in augmentation_table]\n",
    "\n",
    "        ood_tables[dataset] = augmentation_table\n",
    "\n",
    "\n",
    "    # Merge the tables\n",
    "    table = []\n",
    "    for method_idx in range(len(id_tables[present_datasets[0]])):\n",
    "        results = [id_tables[dataset][method_idx] + ood_tables[dataset][method_idx] for dataset in present_datasets]\n",
    "        # flatten the list\n",
    "        results = [item for sublist in results for item in sublist]\n",
    "        table.append(results)\n",
    "\n",
    "    print('Metric: {}'.format(metric))\n",
    "    headers = [\"Method\"]\n",
    "    for dataset in present_datasets:\n",
    "        headers += [dataset + ': ID']\n",
    "        headers += [dataset + ': OOD']\n",
    "\n",
    "    # Print the table\n",
    "    print(tabulate.tabulate(table, headers=headers, tablefmt=\"latex_raw\", floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rank_table_multiple_datasets(group_results_dict, metric=\"error\", no_print=False, raw_ranks=False):\n",
    "    ordered_datasets = [\"svhn\", \"cifar10\", \"cifar100\", \"tinyimagenet\", \"newsgroup\", \"sst\", \"rotated_cifar100\", \"wiki_face\", \"regression_energy\", \"regression_boston\", \"regression_wine\", \"regression_yacht\", \"regression_concrete\", \"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]\n",
    "    present_datasets = []\n",
    "    id_tables = {}\n",
    "    ood_tables = {}\n",
    "    first_dataset = True\n",
    "\n",
    "    for dataset in ordered_datasets:\n",
    "        if dataset not in group_results_dict:\n",
    "            continue\n",
    "        present_datasets.append(dataset)\n",
    "        results_dict = group_results_dict[dataset]\n",
    "        assert dataset == list(results_dict.keys())[0].split('-')[0]\n",
    "\n",
    "        label = '-'.join(list(results_dict.keys())[0].split('-')[:2])\n",
    "    \n",
    "        label_mapping = {\n",
    "        'vanilla': 'No Noise',\n",
    "        'target_smoothing': 'Label Smoothing',\n",
    "        'input_ods': 'Input ODS',\n",
    "        'input_augmix': 'Input AugMix',\n",
    "        'input_target_mixup': 'Input-Target MixUp',\n",
    "        'input_target_cmixup': 'Input-Target CMixUp',\n",
    "        'activation_dropout': 'Activation Dropout',\n",
    "        'gradient_gaussian': 'Gradient Gaussian',\n",
    "        'model_sp': 'Model',\n",
    "        'top2direct': 'Top-2 Direct',\n",
    "        'top3direct': 'Top-3 Direct', \n",
    "        'top2opt': 'Top-2 Optimised',\n",
    "        'top3opt': 'Top-3 Optimised',\n",
    "        'input_additive_gaussian': 'Input Gaussian',\n",
    "        'activation_additive_gaussian': 'Activation Gaussian',\n",
    "        'input_random_crop_horizontal_flip': 'Input Weak Aug.',\n",
    "        'weight_additive_gaussian': 'Weight Gaussian',\n",
    "        'weight_dropconnect': 'Weight DropConnect',\n",
    "        }\n",
    "\n",
    "        ordered_labels = ['No Noise', 'Input Weak Aug.', 'Input Gaussian', 'Input ODS', 'Input AugMix', 'Input-Target MixUp', 'Input-Target CMixUp', 'Label Smoothing', 'Activation Gaussian', 'Activation Dropout', 'Gradient Gaussian', 'Model', 'Weight Gaussian', 'Weight DropConnect', 'Top-2 Direct', 'Top-3 Direct', 'Top-2 Optimised', 'Top-3 Optimised']\n",
    "        \n",
    "        relabeled_results = {}\n",
    "        for key in results_dict:\n",
    "            for label_key in label_mapping:\n",
    "                if label_key in key:\n",
    "                    new_key = label_key\n",
    "            relabeled_results[label_mapping[new_key]] = results_dict[key]\n",
    "\n",
    "        # maintain the preferred order of labels\n",
    "        labels = []\n",
    "        for key in ordered_labels:\n",
    "            if key in relabeled_results:\n",
    "                labels.append(key)\n",
    "\n",
    "        results = relabeled_results\n",
    "\n",
    "        splits = [\"test\"]\n",
    "        _, _, _, levels, augmentation_types = get_dataset_options(dataset)\n",
    "        for i, augmentation in enumerate(augmentation_types):\n",
    "            for level in levels[i]:\n",
    "                splits += [\"{}_{}\".format(augmentation, level)]\n",
    "\n",
    "        # Cache the first table because it will be for the test split\n",
    "        first_table = None\n",
    "        for split in splits:\n",
    "            # Create a table which will compare all the methods on a split with the metric\n",
    "            # The first column will be the method names\n",
    "            table = [[label] for label in labels]\n",
    "\n",
    "            for label, result in results.items():\n",
    "                datapoint = result[metric][split]\n",
    "                if isinstance(datapoint, float):\n",
    "                    # This stands for no standard deviation, since the result was not averaged\n",
    "                    datapoint = (datapoint, 0.0)\n",
    "\n",
    "                table[labels.index(label)].append(datapoint[0])\n",
    "                    \n",
    "            if first_table is None:\n",
    "                first_table = table\n",
    "        \n",
    "        if first_dataset:\n",
    "            id_tables[dataset] = first_table\n",
    "        else:\n",
    "            # remove the first column from the table\n",
    "            first_table = [row[1:] for row in first_table]\n",
    "            id_tables[dataset] = first_table\n",
    "        first_dataset = False\n",
    "\n",
    "        # This is going to aggregate all the augmentations together\n",
    "        # Again create a table which will compare all the methods on all the splits together with all the metrics\n",
    "        table = [[label] for label in labels]\n",
    "\n",
    "        splits = [\"ood_test\"]\n",
    "        for split in splits:\n",
    "            # Create a table which will compare all the methods on a split with the metric\n",
    "            # The first column will be the method names\n",
    "            table = [[label] for label in labels]\n",
    "\n",
    "            for label, result in results.items():\n",
    "                datapoint = result[metric][split]\n",
    "\n",
    "                if isinstance(datapoint, float):\n",
    "                    # This stands for no standard deviation, since the result was not averaged\n",
    "                    datapoint = (datapoint, 0.0)\n",
    "\n",
    "                # check if nan\n",
    "                if datapoint[0] != datapoint[0]:\n",
    "                    table[labels.index(label)].append(9999.9)    \n",
    "                else:\n",
    "                    table[labels.index(label)].append(datapoint[0])    \n",
    "\n",
    "        # Merge the last table with the first one\n",
    "        augmentation_table = table\n",
    "        # Remove the first column from the augmentation table\n",
    "        augmentation_table = [row[1:] for row in augmentation_table]\n",
    "\n",
    "        ood_tables[dataset] = augmentation_table\n",
    "\n",
    "\n",
    "    # Merge the tables\n",
    "    table = []\n",
    "    for method_idx in range(len(id_tables[present_datasets[0]])):\n",
    "        results = [id_tables[dataset][method_idx] + ood_tables[dataset][method_idx] for dataset in present_datasets]\n",
    "        # flatten the list\n",
    "        results = [item for sublist in results for item in sublist]\n",
    "        table.append(results)\n",
    "\n",
    "    \n",
    "    headers = [\"Method\"]\n",
    "    for dataset in present_datasets:\n",
    "        headers += [dataset + ': ID']\n",
    "        headers += [dataset + ': OOD']\n",
    "\n",
    "    # Print the table\n",
    "    results_pd = pd.DataFrame(table, columns=headers)\n",
    "    # calculate the rankings for each column\n",
    "    for column in results_pd.columns[1:]:\n",
    "        results_pd[column] = results_pd[column].astype(float)\n",
    "        results_pd[column] = results_pd[column].rank(ascending=True)\n",
    "\n",
    "    # print results_pd with no index\n",
    "    results_pd_string = results_pd.to_string(index=False)\n",
    "\n",
    "    # remove initial spaces from each row\n",
    "    results_pd_string = '\\n'.join([row.strip() for row in results_pd_string.split('\\n')])\n",
    "\n",
    "    # replace 2 and more spaces with a tab\n",
    "    results_pd_string = '\\n'.join(['\\t'.join([word for word in row.split('  ') if word != '']) for row in results_pd_string.split('\\n')])\n",
    "\n",
    "    if no_print is False:\n",
    "        print()\n",
    "        print('Metric: {}'.format(metric))\n",
    "        print(results_pd_string)\n",
    "\n",
    "    if raw_ranks:\n",
    "        return results_pd\n",
    "\n",
    "    # subtract ranking of No Noise from all other rankings but keep the column Method\n",
    "    # remember the content of the first column\n",
    "    first_column = results_pd.iloc[:, 0]\n",
    "    results_pd = results_pd.iloc[:, 1:]\n",
    "\n",
    "    # subtract the first row from all other rows\n",
    "    results_pd = results_pd - results_pd.iloc[0]\n",
    "\n",
    "    # add again the first column\n",
    "    results_pd.insert(0, 'Method', first_column)\n",
    "\n",
    "    # delete the first row\n",
    "    results_pd = results_pd.iloc[1:]\n",
    "\n",
    "    # calculate the average ranking across columns that have ID in their name\n",
    "    results_pd['ID'] = results_pd[[col for col in results_pd.columns if 'ID' in col]].mean(axis=1)\n",
    "    # format this to one decimal place\n",
    "    results_pd['ID'] = results_pd['ID'].apply(lambda x: round(x, 1))\n",
    "\n",
    "    # calculate the average ranking across columns that have OOD in their name\n",
    "    results_pd['OOD'] = results_pd[[col for col in results_pd.columns if 'OOD' in col]].mean(axis=1)\n",
    "    # format this to one decimal place\n",
    "    results_pd['OOD'] = results_pd['OOD'].apply(lambda x: round(x, 1))\n",
    "\n",
    "    return results_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Results for TMLR Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary_names = os.listdir(\"result_summaries\")\n",
    "result_summary_names = [name for name in result_summary_names if 'transfer' not in name]\n",
    "\n",
    "# get the dataset names\n",
    "dataset_names = set([name.split('-')[0] for name in result_summary_names])\n",
    "\n",
    "dataset_experiments_dict = {}\n",
    "for dataset_name in dataset_names:\n",
    "    if dataset_name == \"newsgroup\" or dataset_name == \"sst\":\n",
    "        experiments_list_cnn = []\n",
    "        experiments_list_transformer = []\n",
    "        # split it based on if the name is \"newsgroup-global_pooling_cnn\" or \"newsgroup-transformer\"\n",
    "        for experiment_name in result_summary_names:\n",
    "            if dataset_name + \"-global_pooling_cnn\" in experiment_name:\n",
    "                experiments_list_cnn.append(experiment_name)\n",
    "            elif dataset_name + \"-transformer\" in experiment_name:\n",
    "                experiments_list_transformer.append(experiment_name)\n",
    "        dataset_experiments_dict[dataset_name + \"-global_pooling_cnn\"] = experiments_list_cnn\n",
    "        dataset_experiments_dict[dataset_name + \"-transformer\"] = experiments_list_transformer\n",
    "    else:\n",
    "        experiments_list = []\n",
    "        for experiment_name in result_summary_names:\n",
    "            if dataset_name == experiment_name.split('-')[0]:\n",
    "                experiments_list.append(experiment_name)\n",
    "        dataset_experiments_dict[dataset_name] = experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"svhn\", \"cifar10\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([error_table['ID'], ece_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['ID'].tolist(), ece_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.009, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_classification_cv_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['svhn'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'ID': error_table['ID'] + ece_table['ID'] + nll_table['ID']})\n",
    "overall_ranks_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"svhn\", \"cifar10\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([error_table['ID'], ece_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['ID'].tolist(), ece_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.01, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_classification_tab_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['classification_wine'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'ID': error_table['ID'] + ece_table['ID'] + nll_table['ID']})\n",
    "overall_ranks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"newsgroup-global_pooling_cnn\", \"sst-global_pooling_cnn\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\")\n",
    "\n",
    "\n",
    "experiment_paths_groups_t = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"newsgroup-transformer\", \"sst-transformer\"]]\n",
    "\n",
    "results_dict_groups_t = load_results_multiple_datasets(experiment_paths_groups_t, root_dir=\"result_summaries\")\n",
    "error_table_t = print_rank_table_multiple_datasets(results_dict_groups_t, metric=\"error\")\n",
    "ece_table_t = print_rank_table_multiple_datasets(results_dict_groups_t, metric=\"ece\")\n",
    "nll_table_t = print_rank_table_multiple_datasets(results_dict_groups_t, metric=\"nll\")\n",
    "\n",
    "# assert the methods are the same\n",
    "assert error_table['Method'].tolist() == error_table_t['Method'].tolist()\n",
    "\n",
    "error_table['ID'] = (error_table['ID'] + error_table_t['ID']) / 2\n",
    "error_table['OOD'] = (error_table['OOD'] + error_table_t['OOD']) / 2\n",
    "\n",
    "ece_table['ID'] = (ece_table['ID'] + ece_table_t['ID']) / 2\n",
    "ece_table['OOD'] = (ece_table['OOD'] + ece_table_t['OOD']) / 2\n",
    "\n",
    "nll_table['ID'] = (nll_table['ID'] + nll_table_t['ID']) / 2\n",
    "nll_table['OOD'] = (nll_table['OOD'] + nll_table_t['OOD']) / 2\n",
    "\n",
    "\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([error_table['ID'], ece_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        error_table_id_list = [\"{:.1f}\".format(e) for e in error_table['ID'].tolist()]\n",
    "        ece_table_id_list = [\"{:.1f}\".format(e) for e in ece_table['ID'].tolist()]\n",
    "        nll_table_id_list = [\"{:.1f}\".format(e) for e in nll_table['ID'].tolist()]\n",
    "        text = ax.text(j, i, [error_table_id_list, ece_table_id_list, nll_table_id_list][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "        \n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.01, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_classification_nlp_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'ID': error_table['ID'] + ece_table['ID'] + nll_table['ID']})\n",
    "overall_ranks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"newsgroup-transformer\", \"sst-transformer\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"newsgroup-global_pooling_cnn\", \"sst-global_pooling_cnn\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"rotated_cifar100\", \"wiki_face\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([mse_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.007, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_regression_cv_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'ID': mse_table['ID'] + nll_table['ID']})\n",
    "overall_ranks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"rotated_cifar100\", \"wiki_face\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "mse_table = print_table_multiple_datasets(results_dict_groups, metric=\"mse\", perc=False)\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\", perc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_energy\", \"regression_boston\", \"regression_wine\", \"regression_yacht\", \"regression_concrete\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([mse_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.008, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_regression_tab_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'ID': mse_table['ID'] + nll_table['ID']})\n",
    "overall_ranks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_energy\", \"regression_boston\", \"regression_wine\", \"regression_yacht\", \"regression_concrete\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "mse_table = print_table_multiple_datasets(results_dict_groups, metric=\"mse\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary_names = os.listdir(\"result_summaries\")\n",
    "result_summary_names = [name for name in result_summary_names if 'transfer' not in name]\n",
    "\n",
    "# get the dataset names\n",
    "dataset_names = set([name.split('-')[0] for name in result_summary_names])\n",
    "\n",
    "dataset_experiments_dict = {}\n",
    "for dataset_name in dataset_names:\n",
    "    if dataset_name == \"newsgroup\" or dataset_name == \"sst\":\n",
    "        experiments_list_cnn = []\n",
    "        experiments_list_transformer = []\n",
    "        # split it based on if the name is \"newsgroup-global_pooling_cnn\" or \"newsgroup-transformer\"\n",
    "        for experiment_name in result_summary_names:\n",
    "            if dataset_name + \"-global_pooling_cnn\" in experiment_name:\n",
    "                experiments_list_cnn.append(experiment_name)\n",
    "            elif dataset_name + \"-transformer\" in experiment_name:\n",
    "                experiments_list_transformer.append(experiment_name)\n",
    "        dataset_experiments_dict[dataset_name + \"-global_pooling_cnn\"] = experiments_list_cnn\n",
    "        dataset_experiments_dict[dataset_name + \"-transformer\"] = experiments_list_transformer\n",
    "    else:\n",
    "        experiments_list = []\n",
    "        for experiment_name in result_summary_names:\n",
    "            if dataset_name == experiment_name.split('-')[0]:\n",
    "                experiments_list.append(experiment_name)\n",
    "        dataset_experiments_dict[dataset_name] = experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"svhn\", \"cifar10\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([error_table['OOD'], ece_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['OOD'].tolist(), ece_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.009, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_classification_cv_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['svhn'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'OOD': error_table['OOD'] + ece_table['OOD'] + nll_table['OOD']})\n",
    "overall_ranks_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"svhn\", \"cifar10\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True, raw_ranks=True)\n",
    "ece_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True, raw_ranks=True)\n",
    "nll_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True, raw_ranks=True)\n",
    "\n",
    "print('Error')\n",
    "dataset_names = error_pd.columns[1:]\n",
    "methods = error_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_error = error_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_error = error_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_error, ood_error)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")\n",
    "\n",
    "# then ECE\n",
    "print('ECE')\n",
    "dataset_names = ece_pd.columns[1:]\n",
    "methods = ece_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_ece = ece_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_ece = ece_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_ece, ood_ece)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")\n",
    "\n",
    "# then NLL\n",
    "print('NLL')\n",
    "dataset_names = nll_pd.columns[1:]\n",
    "methods = nll_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_nll = nll_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_nll = nll_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_nll, ood_nll)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([error_table['OOD'], ece_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['OOD'].tolist(), ece_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.01, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_classification_tab_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['classification_wine'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'OOD': error_table['OOD'] + ece_table['OOD'] + nll_table['OOD']})\n",
    "overall_ranks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True, raw_ranks=True)\n",
    "ece_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True, raw_ranks=True)\n",
    "nll_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True, raw_ranks=True)\n",
    "\n",
    "print('Error')\n",
    "dataset_names = error_pd.columns[1:]\n",
    "methods = error_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_error = error_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_error = error_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_error, ood_error)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")\n",
    "\n",
    "# then ECE\n",
    "print('ECE')\n",
    "dataset_names = ece_pd.columns[1:]\n",
    "methods = ece_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_ece = ece_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_ece = ece_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_ece, ood_ece)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")\n",
    "\n",
    "# then NLL\n",
    "print('NLL')\n",
    "dataset_names = nll_pd.columns[1:]\n",
    "methods = nll_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_nll = nll_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_nll = nll_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_nll, ood_nll)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"rotated_cifar100\", \"wiki_face\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([mse_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.007, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_regression_cv_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'OOD': mse_table['OOD'] + nll_table['OOD']})\n",
    "overall_ranks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"rotated_cifar100\", \"wiki_face\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True, raw_ranks=True)\n",
    "nll_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True, raw_ranks=True)\n",
    "\n",
    "# calculate kendalltau for each dataset between ID and OOD. Do this first for MSE and then for NLL\n",
    "# the column names are Method and then many columns with dataset_name: ID or OOD\n",
    "\n",
    "# first MSE\n",
    "print('MSE')\n",
    "dataset_names = mse_pd.columns[1:]\n",
    "methods = mse_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_mse = mse_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_mse = mse_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_mse, ood_mse)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")\n",
    "\n",
    "# then NLL\n",
    "print('NLL')\n",
    "dataset_names = nll_pd.columns[1:]\n",
    "methods = nll_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_nll = nll_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_nll = nll_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_nll, ood_nll)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_energy\", \"regression_boston\", \"regression_wine\", \"regression_yacht\", \"regression_concrete\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "im = ax.imshow([mse_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.008, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_regression_tab_top.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'OOD': mse_table['OOD'] + nll_table['OOD']})\n",
    "overall_ranks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_energy\", \"regression_boston\", \"regression_wine\", \"regression_yacht\", \"regression_concrete\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True, raw_ranks=True)\n",
    "nll_pd = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True, raw_ranks=True)\n",
    "\n",
    "# calculate kendalltau for each dataset between ID and OOD. Do this first for MSE and then for NLL\n",
    "# the column names are Method and then many columns with dataset_name: ID or OOD\n",
    "\n",
    "# first MSE\n",
    "print('MSE')\n",
    "dataset_names = mse_pd.columns[1:]\n",
    "methods = mse_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_mse = mse_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_mse = mse_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_mse, ood_mse)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")\n",
    "\n",
    "# then NLL\n",
    "print('NLL')\n",
    "dataset_names = nll_pd.columns[1:]\n",
    "methods = nll_pd[\"Method\"].values\n",
    "# filter the dataset names\n",
    "dataset_names = [x.split(\":\")[0] for x in dataset_names]\n",
    "# remove duplicities but keep the order\n",
    "dataset_names = list(dict.fromkeys(dataset_names))\n",
    "\n",
    "tau_list = []\n",
    "for dataset_name in dataset_names:\n",
    "    id_nll = nll_pd[f\"{dataset_name}: ID\"].values\n",
    "    ood_nll = nll_pd[f\"{dataset_name}: OOD\"].values\n",
    "    tau, p_value = kendalltau(id_nll, ood_nll)\n",
    "    print(f\"{dataset_name}: {tau:.3f} ({p_value:.3f})\")\n",
    "    tau_list.append(tau)\n",
    "print(f\"Average: {np.mean(tau_list):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and architecture transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary_names = os.listdir(\"result_summaries\")\n",
    "result_summary_names = [name for name in result_summary_names if 'dataset-transfer' in name]\n",
    "\n",
    "# get the dataset names\n",
    "dataset_names = set([name.split('-')[0] for name in result_summary_names])\n",
    "\n",
    "dataset_experiments_dict = {}\n",
    "for dataset_name in dataset_names:\n",
    "    experiments_list = []\n",
    "    for experiment_name in result_summary_names:\n",
    "        if dataset_name == experiment_name.split('-')[0]:\n",
    "            experiments_list.append(experiment_name)\n",
    "    dataset_experiments_dict[dataset_name] = experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"cifar10\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([error_table['ID'], ece_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['ID'].tolist(), ece_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.012, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_classification_cv_dataset_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['cifar100'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'ID': error_table['ID'] + ece_table['ID'] + nll_table['ID']})\n",
    "\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_energy\", \"regression_wine\", \"regression_concrete\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([mse_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.012, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_regression_tab_dataset_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'ID': mse_table['ID'] + nll_table['ID']})\n",
    "mse_table = print_table_multiple_datasets(results_dict_groups, metric=\"mse\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"cifar10\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([error_table['OOD'], ece_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['OOD'].tolist(), ece_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.012, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_classification_cv_dataset_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['cifar100'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'OOD': error_table['OOD'] + ece_table['OOD'] + nll_table['OOD']})\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_energy\", \"regression_wine\", \"regression_concrete\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([mse_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.012, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_regression_tab_dataset_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'OOD': mse_table['OOD'] + nll_table['OOD']})\n",
    "mse_table = print_table_multiple_datasets(results_dict_groups, metric=\"mse\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary_names = os.listdir(\"result_summaries\")\n",
    "result_summary_names = [name for name in result_summary_names if 'architecture-transfer' in name]\n",
    "\n",
    "# get the dataset names\n",
    "dataset_names = set([name.split('-')[0] for name in result_summary_names])\n",
    "\n",
    "dataset_experiments_dict = {}\n",
    "for dataset_name in dataset_names:\n",
    "    experiments_list = []\n",
    "    for experiment_name in result_summary_names:\n",
    "        if dataset_name == experiment_name.split('-')[0]:\n",
    "            experiments_list.append(experiment_name)\n",
    "    dataset_experiments_dict[dataset_name] = experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"svhn\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([error_table['ID'], ece_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['ID'].tolist(), ece_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.013, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_classification_cv_arch_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['cifar100'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'ID': error_table['ID'] + ece_table['ID'] + nll_table['ID']})\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_concrete\", \"regression_boston\", \"regression_yacht\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([mse_table['ID'], nll_table['ID']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['ID'].tolist(), nll_table['ID'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.012, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/id_regression_tab_arch_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'ID': mse_table['ID'] + nll_table['ID']})\n",
    "mse_table = print_table_multiple_datasets(results_dict_groups, metric=\"mse\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"svhn\", \"cifar100\", \"tinyimagenet\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "error_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"error\", no_print=True)\n",
    "ece_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"ece\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in error_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "# create heatmap with the titles as columns and errors, eces and nlls as rows\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([error_table['OOD'], ece_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(3))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['Error','ECE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [error_table['OOD'].tolist(), ece_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.013, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_classification_cv_arch_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "assert len(results_dict_groups['cifar100'].keys()) - 1 == len(error_table['Method'].tolist())\n",
    "overall_ranks_df = pd.DataFrame({'Method': error_table['Method'], 'OOD': error_table['OOD'] + ece_table['OOD'] + nll_table['OOD']})\n",
    "error_table = print_table_multiple_datasets(results_dict_groups, metric=\"error\")\n",
    "ece_table = print_table_multiple_datasets(results_dict_groups, metric=\"ece\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name]) for dataset_name in [\"regression_concrete\", \"regression_boston\", \"regression_yacht\"]]\n",
    "\n",
    "results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "mse_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"mse\", no_print=True)\n",
    "nll_table = print_rank_table_multiple_datasets(results_dict_groups, metric=\"nll\", no_print=True)\n",
    "\n",
    "titles = []\n",
    "for title in mse_table['Method'].tolist():\n",
    "    titles.append(title)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "im = ax.imshow([mse_table['OOD'], nll_table['OOD']], cmap=my_gradient, vmin=-10.2, vmax=10.2)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(titles)))\n",
    "ax.set_yticks(np.arange(2))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(titles)\n",
    "ax.set_yticklabels(['MSE','NLL'])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(2):\n",
    "    for j in range(len(titles)):\n",
    "        text = ax.text(j, i, [mse_table['OOD'].tolist(), nll_table['OOD'].tolist()][i][j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "# make the colorbar same height as the heatmap\n",
    "cbar = ax.figure.colorbar(im, ax=ax, fraction=0.012, pad=0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('analysis_images/ood_regression_tab_arch_transfer.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "overall_ranks_df = pd.DataFrame({'Method': mse_table['Method'], 'OOD': mse_table['OOD'] + nll_table['OOD']})\n",
    "mse_table = print_table_multiple_datasets(results_dict_groups, metric=\"mse\")\n",
    "nll_table = print_table_multiple_datasets(results_dict_groups, metric=\"nll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative Datasets Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# despine plot from top and right and left - global setting\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(dataset_experiments_dict, dataset_name, save_name, ood=False):\n",
    "    colour_mapping = {\n",
    "        \"No Noise\": \"#4285f4\",\n",
    "        \"Input Weak Aug.\": \"#ea4335\",\n",
    "        \"Input Gaussian\": \"#fbbc04\",\n",
    "        \"Input ODS\": \"#34a853\",\n",
    "        \"Input AugMix\": \"#ff6d01\",\n",
    "        \"Input-Target MixUp\": \"#46bdc6\",\n",
    "        \"Input-Target CMixUp\": \"#46bdc6\",\n",
    "        \"Label Smoothing\": \"#7baaf7\",\n",
    "        \"Activation Gaussian\": \"#f07b72\",\n",
    "        \"Activation Dropout\": \"#fcd04f\",\n",
    "        \"Gradient Gaussian\": \"#71c287\",\n",
    "        \"Model\": \"#ff994d\",\n",
    "        \"Weight Gaussian\": \"#7ed1d7\",\n",
    "        \"Weight DropConnect\": \"#b3cefb\",\n",
    "        \"Top-2 Direct\": \"#f7b4ae\",\n",
    "        \"Top-3 Direct\": \"#fde49b\",\n",
    "        \"Top-2 Optimised\": \"#aedcba\",\n",
    "        \"Top-3 Optimised\": \"#ffc599\",\n",
    "    }\n",
    "\n",
    "    if \"regression\" in dataset_name or \"wiki_face\" in dataset_name or \"rotated\" in dataset_name:\n",
    "        problem = \"regression\"\n",
    "        metrics = [\"mse\", \"nll\"]\n",
    "    else:\n",
    "        problem = \"classification\"\n",
    "        metrics = [\"error\", \"ece\"]\n",
    "\n",
    "    if ood:\n",
    "        split = \"ood_test\"\n",
    "    else:\n",
    "        split = \"test\"\n",
    "\n",
    "    experiment_paths_groups = [\"\\n\".join(dataset_experiments_dict[dataset_name])]\n",
    "    results_dict_groups = load_results_multiple_datasets(experiment_paths_groups, root_dir=\"result_summaries\")\n",
    "\n",
    "    label_mapping = {\n",
    "        'vanilla': 'No Noise',\n",
    "        'target_smoothing': 'Label Smoothing',\n",
    "        'input_ods': 'Input ODS',\n",
    "        'input_augmix': 'Input AugMix',\n",
    "        'input_target_mixup': 'Input-Target MixUp',\n",
    "        'activation_dropout': 'Activation Dropout',\n",
    "        'gradient_gaussian': 'Gradient Gaussian',\n",
    "        'model_sp': 'Model',\n",
    "        'top2direct': 'Top-2 Direct',\n",
    "        'top3direct': 'Top-3 Direct', \n",
    "        'top2opt': 'Top-2 Optimised',\n",
    "        'top3opt': 'Top-3 Optimised',\n",
    "        'input_additive_gaussian': 'Input Gaussian',\n",
    "        'input_target_cmixup': 'Input-Target CMixUp',\n",
    "        'activation_additive_gaussian': 'Activation Gaussian',\n",
    "        'input_random_crop_horizontal_flip': 'Input Weak Aug.',\n",
    "        'weight_additive_gaussian': 'Weight Gaussian',\n",
    "        'weight_dropconnect': 'Weight DropConnect',\n",
    "        }\n",
    "\n",
    "    ordered_labels = ['No Noise', 'Input Weak Aug.', 'Input Gaussian', 'Input ODS', 'Input AugMix', 'Input-Target MixUp', 'Input-Target CMixUp', 'Label Smoothing', 'Activation Gaussian', 'Activation Dropout', 'Gradient Gaussian', 'Model', 'Weight Gaussian', 'Weight DropConnect', 'Top-2 Direct', 'Top-3 Direct', 'Top-2 Optimised', 'Top-3 Optimised']\n",
    "            \n",
    "    relabeled_results = {}\n",
    "    for key in results_dict_groups[dataset_name]:\n",
    "        for label_key in label_mapping:\n",
    "            if label_key in key:\n",
    "                new_key = label_key\n",
    "        relabeled_results[label_mapping[new_key]] = results_dict_groups[dataset_name][key]\n",
    "\n",
    "    labels = []\n",
    "    selected_colours = []\n",
    "    for key in ordered_labels:\n",
    "        if key in relabeled_results:\n",
    "            labels.append(key)\n",
    "            selected_colours.append(colour_mapping[key])\n",
    "\n",
    "    results_per_metric = {metric: {} for metric in metrics}\n",
    "    if problem == \"classification\":\n",
    "        for label in labels:\n",
    "            for metric in metrics:\n",
    "                results_per_metric[metric][label] = float(relabeled_results[label][metric][split][0]) * 100\n",
    "    else:\n",
    "        for label in labels:\n",
    "            for metric in metrics:\n",
    "                results_per_metric[metric][label] = float(relabeled_results[label][metric][split][0])\n",
    "\n",
    "    error_table = results_per_metric[metrics[0]]\n",
    "    ece_table = results_per_metric[metrics[1]]\n",
    "\n",
    "    # make a plot with two subplots - error first and then ece as next column\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(error_table)), list(error_table.values()), align='center', color=selected_colours)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks(fontsize=14)\n",
    "    if problem == \"classification\":\n",
    "        plt.ylabel(\"Error (%)\", fontsize=16)\n",
    "    else:\n",
    "        plt.ylabel(\"MSE\", fontsize=16)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(len(ece_table)), list(ece_table.values()), align='center', color=selected_colours)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks(fontsize=14)\n",
    "    if problem == \"classification\":\n",
    "        plt.ylabel(\"ECE (%)\", fontsize=16)\n",
    "    else:\n",
    "        plt.ylabel(\"NLL\", fontsize=16)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(\"selected_performances/\" + save_name + \".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_summary_names = os.listdir(\"result_summaries\")\n",
    "result_summary_names = [name for name in result_summary_names if 'transfer' not in name]\n",
    "\n",
    "# get the dataset names\n",
    "dataset_names = set([name.split('-')[0] for name in result_summary_names])\n",
    "\n",
    "dataset_experiments_dict = {}\n",
    "for dataset_name in dataset_names:\n",
    "    if dataset_name == \"newsgroup\" or dataset_name == \"sst\":\n",
    "        experiments_list_cnn = []\n",
    "        experiments_list_transformer = []\n",
    "        # split it based on if the name is \"newsgroup-global_pooling_cnn\" or \"newsgroup-transformer\"\n",
    "        for experiment_name in result_summary_names:\n",
    "            if dataset_name + \"-global_pooling_cnn\" in experiment_name:\n",
    "                experiments_list_cnn.append(experiment_name)\n",
    "            elif dataset_name + \"-transformer\" in experiment_name:\n",
    "                experiments_list_transformer.append(experiment_name)\n",
    "        dataset_experiments_dict[dataset_name + \"-global_pooling_cnn\"] = experiments_list_cnn\n",
    "        dataset_experiments_dict[dataset_name + \"-transformer\"] = experiments_list_transformer\n",
    "    else:\n",
    "        experiments_list = []\n",
    "        for experiment_name in result_summary_names:\n",
    "            if dataset_name == experiment_name.split('-')[0]:\n",
    "                experiments_list.append(experiment_name)\n",
    "        dataset_experiments_dict[dataset_name] = experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"cifar10\", \"id_selection_c10\", ood=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"cifar10\", \"ood_selection_c10\", ood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot({'newsgroup': dataset_experiments_dict['newsgroup-global_pooling_cnn']}, \"newsgroup\", \"id_selection_ng\", ood=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"classification_adult\", \"id_selection_cls_adult\", ood=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"classification_adult\", \"ood_selection_cls_adult\", ood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"wiki_face\", \"id_selection_wiki_face\", ood=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"wiki_face\", \"ood_selection_wiki_face\", ood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"regression_yacht\", \"id_selection_regr_yacht\", ood=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot(dataset_experiments_dict, \"regression_yacht\", \"ood_selection_regr_yacht\", ood=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
