{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Dataset Out-of-Distribution Augmentations\n",
    "\n",
    "In this notebook we examine the proposed out-of-distribution augmentations for tabular datasets and evaluate how often a sample under augmentation would have a nearest neighbor different from the original sample.\n",
    "\n",
    "We do this to verify that the proposed augmentations are not too aggressive so that a sample under augmentation would be closer to the original sample or to the sample from the same class than to a different sample from a different class. We identify scaling factors for each dataset to ensure that the proposed augmentations are not too aggressive which could result in augmentations that would change the data distribution too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 13:55:55.844169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 13:55:56.398593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "\n",
    "from src.data.uci import UCI\n",
    "from src.third_party.corruptions import TabularCorruption, TABULAR_MULTIPLICATIVE_SCALE, TABULAR_ADDITIVE_SCALE\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "DATASETS = [\"regression_concrete\", \"regression_boston\", \"regression_energy\", \"regression_wine\", \"regression_yacht\", \"classification_wine\", \"classification_toxicity\", \"classification_abalone\", \"classification_students\", \"classification_adult\"]\n",
    "DATA_ROOT = \"~/.torch\"\n",
    "TEST_PORTION = 0.0 \n",
    "SEEDS = 3\n",
    "CORRUPTION_LEVELS = TabularCorruption.levels\n",
    "CORRUPTION_NAMES = TabularCorruption.corruption_names\n",
    "VANILLA_TABULAR_MULTIPLICATIVE_SCALE = copy.deepcopy(TABULAR_MULTIPLICATIVE_SCALE)\n",
    "VANILLA_TABULAR_ADDITIVE_SCALE = copy.deepcopy(TABULAR_ADDITIVE_SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_match_after_corruptions(datasets=DATASETS, seeds=SEEDS, corruption_levels=CORRUPTION_LEVELS, corruption_names=CORRUPTION_NAMES):\n",
    "    results = {}\n",
    "    for seed in range(seeds):\n",
    "        if seed not in results:\n",
    "            results[seed] = {}\n",
    "        # Fix the seed\n",
    "        np.random.seed(seed)\n",
    "        for dataset in datasets:\n",
    "            if dataset not in results[seed]:\n",
    "                results[seed][dataset] = {}\n",
    "            task, dataset_name = dataset.split(\"_\")\n",
    "            clean_dataset = UCI(dataset_name, root=DATA_ROOT, task=task, train=True, test_portion=TEST_PORTION, transform=None)\n",
    "            clean_xy = [(x, y) for x, y in clean_dataset]\n",
    "            clean_x = np.array([x.numpy() for x, _ in clean_xy]).reshape(-1, clean_xy[0][0].shape[0])\n",
    "            clean_y = np.array([y.numpy() for _, y in clean_xy]).reshape(-1, 1)            \n",
    "            \n",
    "            # Train a KNN classifier on the clean data\n",
    "            clean_knn = KNeighborsClassifier(n_neighbors=1) if task == \"classification\" else KNeighborsRegressor(n_neighbors=1)\n",
    "            clean_knn.fit(clean_x, clean_y.ravel())\n",
    "            \n",
    "            for level in range(corruption_levels):\n",
    "                if level not in results[seed][dataset]:\n",
    "                    results[seed][dataset][level] = {}\n",
    "                for name in corruption_names:\n",
    "                    if name not in results[seed][dataset][level]:\n",
    "                        results[seed][dataset][level][name] = {}\n",
    "                        \n",
    "                    aug_dataset = UCI(dataset_name, root=DATA_ROOT, task=task, train=True, test_portion=TEST_PORTION, transform=TabularCorruption(name, level, dataset_scale=1.0))\n",
    "                    aug_xy = [(x, y) for x, y in aug_dataset]\n",
    "                    aug_x, aug_y = np.array([x.numpy() for x, _ in aug_xy]), np.array([y.numpy() for _, y in aug_xy])\n",
    "                    aug_x = aug_x.reshape(-1, aug_xy[0][0].shape[0])\n",
    "                    aug_y = aug_y.reshape(-1, 1)\n",
    "                    \n",
    "                    pred_y = clean_knn.predict(aug_x)\n",
    "                    \n",
    "                    if task == \"classification\":\n",
    "                        acc = np.mean(pred_y.ravel() == aug_y.ravel())\n",
    "                        results[seed][dataset][level][name] = acc\n",
    "                    else:\n",
    "                        mse = np.mean((pred_y.ravel() - aug_y.ravel()) ** 2)\n",
    "                        results[seed][dataset][level][name] = mse\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_results(results):\n",
    "    \"\"\"Create a table of results.\n",
    "    \n",
    "    Analyse the result across all datasets with respect to each corruption level across all corruption types separately for each task.\n",
    "    Analyse the result across all datasets with respect to each corruption type across all corruption levels separately for each task.\n",
    "    Analyse the result across all options separately for each task.\n",
    "    \"\"\"\n",
    "    for task in [\"regression\", \"classification\"]:\n",
    "        print(f\"Task: {task}\")\n",
    "        print(\"Level\\t\" + \"\\t\".join([str(level) for level in range(CORRUPTION_LEVELS)]))\n",
    "        for name in CORRUPTION_NAMES:\n",
    "            print(f\"{name}\\t\" + \"\\t\".join([f\"{np.mean([results[seed][dataset][level][name] for seed in range(SEEDS) for dataset in DATASETS if dataset.startswith(task)])} +- {np.std([results[seed][dataset][level][name] for seed in range(SEEDS) for dataset in DATASETS if dataset.startswith(task)])}\" for level in range(CORRUPTION_LEVELS)]))\n",
    "        print(\"Average\\t\" + \"\\t\".join([f\"{np.mean([results[seed][dataset][level][name] for seed in range(SEEDS) for dataset in DATASETS if dataset.startswith(task) for name in CORRUPTION_NAMES])} +- {np.std([results[seed][dataset][level][name] for seed in range(SEEDS) for dataset in DATASETS if dataset.startswith(task) for name in CORRUPTION_NAMES])}\" for level in range(CORRUPTION_LEVELS)]))\n",
    "        print()\n",
    "        print(\"Complete average\\t\" + f\"{np.mean([results[seed][dataset][level][name] for seed in range(SEEDS) for dataset in DATASETS if dataset.startswith(task) for name in CORRUPTION_NAMES for level in range(CORRUPTION_LEVELS)])} +- {np.std([results[seed][dataset][level][name] for seed in range(SEEDS) for dataset in DATASETS if dataset.startswith(task) for name in CORRUPTION_NAMES for level in range(CORRUPTION_LEVELS)])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: regression\n",
      "Level\t0\t1\t2\t3\t4\n",
      "additive_gaussian_noise\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\n",
      "multiplicative_gaussian_noise\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\n",
      "additive_uniform_noise\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\n",
      "multiplicative_uniform_noise\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\n",
      "multiplicative_bernoulli_noise\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\t0.0008100830600596964 +- 0.0016191720496863127\n",
      "Average\t0.0008100831182673573 +- 0.0016191721661016345\t0.0008100831182673573 +- 0.0016191721661016345\t0.0008100831182673573 +- 0.0016191721661016345\t0.0008100831182673573 +- 0.0016191721661016345\t0.0008100831182673573 +- 0.0016191721661016345\n",
      "\n",
      "Complete average\t0.0008100831764750183 +- 0.0016191720496863127\n",
      "Task: classification\n",
      "Level\t0\t1\t2\t3\t4\n",
      "additive_gaussian_noise\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\n",
      "multiplicative_gaussian_noise\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\n",
      "additive_uniform_noise\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\n",
      "multiplicative_uniform_noise\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\n",
      "multiplicative_bernoulli_noise\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\t0.9999938576825038 +- 1.2284634992765219e-05\n",
      "Average\t0.9999938576825035 +- 1.2284634992765219e-05\t0.9999938576825035 +- 1.2284634992765219e-05\t0.9999938576825035 +- 1.2284634992765219e-05\t0.9999938576825035 +- 1.2284634992765219e-05\t0.9999938576825035 +- 1.2284634992765219e-05\n",
      "\n",
      "Complete average\t0.9999938576825036 +- 1.228463499276522e-05\n"
     ]
    }
   ],
   "source": [
    "# Perform a sanity check by changing the severity of the corruption to all zeros\n",
    "for i in range(len(TABULAR_MULTIPLICATIVE_SCALE)):\n",
    "    TABULAR_MULTIPLICATIVE_SCALE[i] = 0.0\n",
    "    TABULAR_ADDITIVE_SCALE[i] = 0.0\n",
    "    \n",
    "results = benchmark_match_after_corruptions(datasets=DATASETS, seeds=SEEDS, corruption_levels=CORRUPTION_LEVELS, corruption_names=CORRUPTION_NAMES)\n",
    "analyse_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: regression\n",
      "Level\t0\t1\t2\t3\t4\n",
      "additive_gaussian_noise\t0.7245924472808838 +- 1.4266492128372192\t0.8565842509269714 +- 1.6084719896316528\t0.9593607783317566 +- 1.6850926876068115\t1.0404561758041382 +- 1.7138861417770386\t1.0950682163238525 +- 1.6524163484573364\n",
      "multiplicative_gaussian_noise\t0.8654730319976807 +- 1.6306952238082886\t1.0199588537216187 +- 1.6769318580627441\t1.1537892818450928 +- 1.6079789400100708\t1.311373233795166 +- 1.6085981130599976\t1.47493577003479 +- 1.5505502223968506\n",
      "additive_uniform_noise\t0.6107798218727112 +- 1.2170313596725464\t0.8011578917503357 +- 1.5698291063308716\t0.8779402375221252 +- 1.6662099361419678\t0.9475632309913635 +- 1.7502315044403076\t0.9661989808082581 +- 1.7197080850601196\n",
      "multiplicative_uniform_noise\t0.7805746793746948 +- 1.5208638906478882\t0.9210113286972046 +- 1.7049763202667236\t0.9961715936660767 +- 1.656866192817688\t1.0976078510284424 +- 1.7251814603805542\t1.1811020374298096 +- 1.6513701677322388\n",
      "multiplicative_bernoulli_noise\t0.2593959867954254 +- 0.13568741083145142\t0.472821444272995 +- 0.2595176100730896\t0.6258441805839539 +- 0.3497260808944702\t0.7877804636955261 +- 0.4435328543186188\t0.974846601486206 +- 0.514674723148346\n",
      "Average\t0.6481631994247437 +- 1.321380615234375\t0.8143067359924316 +- 1.48392653465271\t0.9226212501525879 +- 1.4979435205459595\t1.0369561910629272 +- 1.5433956384658813\t1.1384303569793701 +- 1.5004903078079224\n",
      "\n",
      "Complete average\t0.9120955467224121 +- 1.481324553489685\n",
      "Task: classification\n",
      "Level\t0\t1\t2\t3\t4\n",
      "additive_gaussian_noise\t0.8877431756021141 +- 0.2089877900551492\t0.8450862669459955 +- 0.21889559731549488\t0.8099281160393065 +- 0.22946048841876163\t0.7867828055565182 +- 0.24084874520112515\t0.7720172296102993 +- 0.24691271676323906\n",
      "multiplicative_gaussian_noise\t0.8429636228602554 +- 0.22045017072709414\t0.7865750908828006 +- 0.24097107582072402\t0.7572970923595083 +- 0.25482152507044586\t0.7416840756821602 +- 0.2596132075908323\t0.7297643628380857 +- 0.25725957251280834\n",
      "additive_uniform_noise\t0.9013763631885178 +- 0.19491275371033348\t0.876756869882008 +- 0.22416069894291388\t0.8519939747621639 +- 0.22374061075162682\t0.8274909670694744 +- 0.22505623610845957\t0.8060297326386489 +- 0.23215878467642284\n",
      "multiplicative_uniform_noise\t0.8779331101636598 +- 0.21994578227985967\t0.827867222341872 +- 0.224043846052852\t0.7894762975861397 +- 0.24198841090677242\t0.7699755282877023 +- 0.25061163879110854\t0.7558423828129766 +- 0.25790242157433696\n",
      "multiplicative_bernoulli_noise\t0.9196198620717467 +- 0.05319806763865371\t0.8327858140885621 +- 0.07932994721813169\t0.7679189872531982 +- 0.09483460550332161\t0.6976071720801634 +- 0.09163271762384612\t0.6431794443422524 +- 0.10178911868013917\n",
      "Average\t0.8859272267772589 +- 0.19222813612419798\t0.8338142528282477 +- 0.20830666637317088\t0.7953228936000634 +- 0.21947848276715248\t0.7647081097352038 +- 0.22660518992983716\t0.7413666304484526 +- 0.23367773618229512\n",
      "\n",
      "Complete average\t0.8042278226778453 +- 0.2225363381123054\n"
     ]
    }
   ],
   "source": [
    "# Run the benchmark on the original corruption strengths\n",
    "for i in range(len(TABULAR_MULTIPLICATIVE_SCALE)):\n",
    "    TABULAR_MULTIPLICATIVE_SCALE[i] = VANILLA_TABULAR_MULTIPLICATIVE_SCALE[i]\n",
    "    TABULAR_ADDITIVE_SCALE[i] = VANILLA_TABULAR_ADDITIVE_SCALE[i]\n",
    "    \n",
    "results = benchmark_match_after_corruptions(datasets=DATASETS, seeds=SEEDS, corruption_levels=CORRUPTION_LEVELS, corruption_names=CORRUPTION_NAMES)\n",
    "analyse_results(results)\n",
    "pickle.dump(results, open(\"./match_after_corruptions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset, try different corruption strengths which are multiples of the original corruption strengths\n",
    "# This enables us to find the corruption strength which is the most suitable for each dataset\n",
    "dataset_results = {}\n",
    "for scaling_factor in np.geomspace(0.0001, 1.0, 20):\n",
    "    for i in range(len(TABULAR_MULTIPLICATIVE_SCALE)):\n",
    "        TABULAR_MULTIPLICATIVE_SCALE[i] = VANILLA_TABULAR_MULTIPLICATIVE_SCALE[i] * scaling_factor\n",
    "        TABULAR_ADDITIVE_SCALE[i] = VANILLA_TABULAR_ADDITIVE_SCALE[i] * scaling_factor\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        if dataset not in dataset_results:\n",
    "            dataset_results[dataset] = {}\n",
    "        dataset_results[dataset][scaling_factor] = benchmark_match_after_corruptions(datasets=[dataset], seeds=SEEDS, corruption_levels=CORRUPTION_LEVELS, corruption_names=CORRUPTION_NAMES)\n",
    "pickle.dump(dataset_results, open(\"./match_after_corruptions_dataset_results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strongest_scaling_factor(dataset_results, dataset, threshold, task):\n",
    "    \"\"\"This function finds the strongest scaling factor for a dataset such that the accuracy is above a threshold.\n",
    "    \n",
    "    For regression, the threshold is the maximum MSE.\n",
    "    \"\"\"\n",
    "    strongest_scaling_factor = 0.0\n",
    "    for scaling_factor in np.geomspace(0.0001, 1.0, 20):\n",
    "        if task == \"classification\":\n",
    "            if np.mean([dataset_results[dataset][scaling_factor][seed][dataset][level][name] for seed in range(SEEDS) for level in range(CORRUPTION_LEVELS) for name in CORRUPTION_NAMES]) > threshold:\n",
    "                strongest_scaling_factor = scaling_factor\n",
    "        if task == \"regression\":\n",
    "            if np.mean([dataset_results[dataset][scaling_factor][seed][dataset][level][name] for seed in range(SEEDS) for level in range(CORRUPTION_LEVELS) for name in CORRUPTION_NAMES]) < threshold:\n",
    "                strongest_scaling_factor = scaling_factor\n",
    "    return strongest_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given thresholds 0.99, 0.98, 0.97, 0.96, 0.95, find the strongest scaling factor for classification datasets\n",
    "# Given thresholds 0.01, 0.02, 0.1, 0.2, 0.3, find the strongest scaling factor for regression datasets\n",
    "classification_thresholds = [0.99, 0.98, 0.97, 0.96, 0.95]\n",
    "regression_thresholds = [0.01, 0.02, 0.1, 0.2, 0.3]\n",
    "strongest_scaling_factors = {} # Dataset -> threshold -> strongest scaling factor\n",
    "for dataset in DATASETS:\n",
    "    strongest_scaling_factors[dataset] = {}\n",
    "    if dataset.startswith(\"classification\"):\n",
    "        for threshold in classification_thresholds:\n",
    "            strongest_scaling_factors[dataset][threshold] = get_strongest_scaling_factor(dataset_results, dataset, threshold, \"classification\")\n",
    "    if dataset.startswith(\"regression\"):\n",
    "        for threshold in regression_thresholds:\n",
    "            strongest_scaling_factors[dataset][threshold] = get_strongest_scaling_factor(dataset_results, dataset, threshold, \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification (thresholds)\n",
      "0.99\t0.98\t0.97\t0.96\t0.95\n",
      "classification_wine\t0.007847599703514606\t0.012742749857031334\t0.012742749857031334\t0.012742749857031334\t0.0206913808111479\n",
      "classification_toxicity\t0.3792690190732246\t0.615848211066026\t0.615848211066026\t1.0\t1.0\n",
      "classification_abalone\t0.08858667904100823\t0.14384498882876628\t0.14384498882876628\t0.14384498882876628\t0.23357214690901212\n",
      "classification_students\t0.23357214690901212\t0.3792690190732246\t0.615848211066026\t0.615848211066026\t1.0\n",
      "classification_adult\t0.14384498882876628\t0.23357214690901212\t0.3792690190732246\t0.3792690190732246\t0.615848211066026\n",
      "\n",
      "Regression (thresholds)\n",
      "0.01\t0.02\t0.1\t0.2\t0.3\n",
      "regression_concrete\t0.0206913808111479\t0.05455594781168514\t0.3792690190732246\t0.615848211066026\t0.615848211066026\n",
      "regression_boston\t0.08858667904100823\t0.14384498882876628\t0.3792690190732246\t0.615848211066026\t1.0\n",
      "regression_energy\t0.05455594781168514\t0.14384498882876628\t0.615848211066026\t1.0\t1.0\n",
      "regression_wine\t0.004832930238571752\t0.007847599703514606\t0.012742749857031334\t0.0206913808111479\t0.0206913808111479\n",
      "regression_yacht\t0.08858667904100823\t0.14384498882876628\t0.3792690190732246\t0.615848211066026\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Print a table of the strongest scaling factors for the thresholds\n",
    "print(\"Classification (thresholds)\")\n",
    "print(\"\\t\".join([str(threshold) for threshold in classification_thresholds]))\n",
    "for dataset in DATASETS:\n",
    "    if dataset.startswith(\"classification\"):\n",
    "        print(dataset + \"\\t\" + \"\\t\".join([str(strongest_scaling_factors[dataset][threshold]) for threshold in classification_thresholds]))\n",
    "print()\n",
    "print(\"Regression (thresholds)\")\n",
    "print(\"\\t\".join([str(threshold) for threshold in regression_thresholds]))\n",
    "for dataset in DATASETS:\n",
    "    if dataset.startswith(\"regression\"):\n",
    "        print(dataset + \"\\t\" + \"\\t\".join([str(strongest_scaling_factors[dataset][threshold]) for threshold in regression_thresholds]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
